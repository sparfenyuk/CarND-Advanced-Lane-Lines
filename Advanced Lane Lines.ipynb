{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lane Finding Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "- Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "- Apply a distortion correction to raw images.\n",
    "- Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "- Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "- Detect lane pixels and fit to find the lane boundary.\n",
    "- Determine the curvature of the lane and vehicle position with respect to center.\n",
    "- Warp the detected lane boundaries back onto the original image.\n",
    "- Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare our basic tools: OpenCV, matplotlib and numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Camera calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "*Briefly state how you computed the camera matrix and distortion coefficients. Provide an example of a distortion corrected calibration image.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin camera calibration after reading all available images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def load_images(load_path):\n",
    "    image_paths = glob.glob(load_path)\n",
    "    images = [plt.imread(path) for path in image_paths]\n",
    "    return images\n",
    "        \n",
    "def display_grid(images, n_col, title, cmap=None, figsize=(15,7)):\n",
    "    plt.close('all')\n",
    "    fig, ax_arr = plt.subplots(len(images)//n_col, n_col, figsize=figsize)\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle(title, fontsize=30)\n",
    "    for i, image in enumerate(images):\n",
    "        ax = ax_arr[i // n_col, i % n_col]\n",
    "        ax.imshow(image,cmap=cmap)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "calibration_images = load_images(\"camera_cal/*.jpg\")\n",
    "print(\"There are {} images for calibrating camera\".format(len(calibration_images)))\n",
    "display_grid(calibration_images, 5, \"Images for camera calibration\", figsize=(15,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above there are 20 images. Using them we could do camera calibration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calibrate camera I have to found out calibration parameters. It is known, that calibration chessboard has pattern size equal to 9x6. That's enough to find chessboard corners with method `cv2.findChessboardCorners`. Then, I setup object points - points in 3D space and call `cv2.calibrateCamera` to finally get parameters `mtx` and `dist` to be abble to undistort any given image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting undistorted images there is a need to transform image perspective. We have to know `corners` which were set in previous step and that's all. Displaying images we can see that some of them weren't undistorted and transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_corners(image, pattern_size):\n",
    "    \"\"\"\n",
    "    Look for the corners on chessboard image given pattern size\n",
    "    \n",
    "    Args:\n",
    "        image: cv2.image\n",
    "        pattern_size: tuple\n",
    "    \n",
    "    Returns:\n",
    "        List of corners coordinates if they are found\n",
    "    \"\"\"\n",
    "    \n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    retval, corners = cv2.findChessboardCorners(gray, pattern_size)\n",
    "    return corners if retval else []\n",
    "\n",
    "def find_calibration_parameters(images, pattern_size):\n",
    "    \"\"\"\n",
    "    Find out calibration parameters. \n",
    "    \n",
    "    Args:\n",
    "        image: cv2.image\n",
    "        pattern_size: tuple\n",
    "    \n",
    "    Returns:\n",
    "        ret: boolean â€“ true if calibration was successfull\n",
    "        rest parameters - camera matrix, distortion coefficients and used array of corners\n",
    "    \"\"\"\n",
    "    return_corners = list(map(lambda x: find_corners(x, pattern_size), images))\n",
    "    corners = list(filter(lambda x: len(x) != 0, return_corners))\n",
    "\n",
    "    objp_size = np.prod(pattern_size), 3\n",
    "    objp = np.zeros(objp_size, np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:pattern_size[0], 0:pattern_size[1]].T.reshape(-1,2)\n",
    "    objpoints = [objp] * len(corners)\n",
    "    \n",
    "    image_size = images[-1].shape[1::-1]\n",
    "    ret, mtx, dist, *rest = cv2.calibrateCamera(objpoints, corners, image_size, None, None)\n",
    "    \n",
    "    return ret, mtx, dist, return_corners\n",
    "\n",
    "def undistort_images(images, mtx, dist):\n",
    "    return [cv2.undistort(image, mtx, dist) for image in images]\n",
    "\n",
    "def draw_vision_area(image, corners):\n",
    "    src = np.float32([corners[0], corners[1], corners[-1], corners[-2]])\n",
    "    image_size = image.shape[1::-1]\n",
    "    pts = np.array(src, np.int32)\n",
    "    pts = pts.reshape((-1,1,2))\n",
    "    return cv2.polylines(image, [pts], True, (255, 0,0), 20)\n",
    "    \n",
    "def transform_perspective(image, corners, pattern_size = (2,2), offset = 0, inverted = False):\n",
    "    if len(corners) == 0:\n",
    "        img = image.copy()\n",
    "        cv2.putText(img, \"Failed\", (10, 100), cv2.FONT_ITALIC, 4.0, (0, 0, 255), 6, cv2.LINE_AA)\n",
    "        return img\n",
    "    nx, ny = pattern_size\n",
    "    src = np.float32([corners[0], corners[nx-1], corners[-1], corners[-nx]])\n",
    "    image_size = image.shape[1::-1]\n",
    "    (w, h), d = image_size, offset\n",
    "    dst = np.float32([[d, d], [w - d, d], [w-d, h-d], [d, h - d]])\n",
    "    M = cv2.getPerspectiveTransform(dst, src) if inverted else cv2.getPerspectiveTransform(src, dst)\n",
    "    return cv2.warpPerspective(image, M, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ps = (9, 6)\n",
    "ret, mtx, dist, corners = find_calibration_parameters(calibration_images, pattern_size=ps)\n",
    "\n",
    "if ret:\n",
    "    undistorted_images = undistort_images(calibration_images, mtx, dist)\n",
    "#     display_grid(undistorted_images, 5, \"Undistorted images\")\n",
    "\n",
    "    transformed_images = [transform_perspective(image, corners[i], pattern_size=ps, offset=100) \\\n",
    "                          for i, image in enumerate(undistorted_images)]\n",
    "    display_grid(transformed_images, 5, \"Undistorted and Transformed images\", figsize=(15,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's interesting to note that some images aren't helpful because the `findChessboardCorners` method cannot detect corners on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Pipeline (for `test_images` folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off all, load images from folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_images = load_images(\"test_images/*.jpg\")\n",
    "display_grid(test_images, 4, \"Test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "\n",
    "*Provide an example of a distortion-corrected image.*\n",
    "\n",
    "\n",
    "*Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do a perspective transform I need to setup corners. All other transformations are done with help of available methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_corners = lambda w, h: [(0.45*w, 0.62*h), (0.55*w, 0.62*h), (0.05*w, 0.95*h), (0.95*w, 0.95*h)]\n",
    "\n",
    "def undistort_and_transform_perspective(images):\n",
    "    undistorted_images = undistort_images(images, mtx, dist)\n",
    "    h, w, *_ = images[-1].shape\n",
    "    corners = get_corners(w, h)\n",
    "    transformed_images = [draw_vision_area(image.copy(), corners) \\\n",
    "                          for i, image in enumerate(undistorted_images)]\n",
    "    display_grid(transformed_images, 4, \"Undistored images with source points drawn\")\n",
    "    \n",
    "    transformed_images = [transform_perspective(image, corners) \\\n",
    "                          for i, image in enumerate(undistorted_images)]\n",
    "    display_grid(transformed_images, 4, \"Birds-eye perspective\")\n",
    "    \n",
    "    transformed_images = [transform_perspective(image, corners, inverted=True) \\\n",
    "                          for i, image in enumerate(undistorted_images)]\n",
    "    display_grid(transformed_images, 4, \"Inversed perspective\")\n",
    "    \n",
    "undistort_and_transform_perspective(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "\n",
    "*Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image. Provide an example of a binary image result.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I convert images into grayscale and then put them into image processing pipeline. The pipeline applies absolute sobel operation with threshold, calculates magnitude of the gradient and it threshold, and then evaluate direction of the gradient and it threshold. Doing all this operation on grayscale image pipeline combine the results into single binary image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gray_test_images = undistort_images([cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in test_images], mtx, dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_sobel_op(gray, sobel_kernel = 3):\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "    return sobelx, sobely\n",
    "\n",
    "def abs_sobel_thresh(sobelx, sobely, orient='x', thresh = (0, 255)):\n",
    "    \n",
    "    abs_sobel = np.absolute(sobelx) if orient == 'x' else np.absolute(sobely)\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "    binary_output[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "\n",
    "    return binary_output\n",
    "\n",
    "def mag_thresh(sobelx, sobely, thresh=(0, 255)):\n",
    "    abs_sobel_xy = np.sqrt(sobelx ** 2 + sobely ** 2)\n",
    "    abs_sobel_xy = (abs_sobel_xy * 255 / np.max(abs_sobel_xy)).astype(np.uint8)\n",
    "\n",
    "    binary_output = np.zeros_like(abs_sobel_xy)\n",
    "    binary_output[(abs_sobel_xy >= thresh[0]) & (abs_sobel_xy <= thresh[1])] = 1\n",
    "\n",
    "    return binary_output\n",
    "\n",
    "\n",
    "def dir_threshold(sobelx, sobely, thresh=(0, np.pi/2)):\n",
    "    abs_sobelx, abs_sobely = np.absolute(sobelx), np.absolute(sobely)\n",
    "\n",
    "    a = np.arctan2(abs_sobely, abs_sobelx)\n",
    "    \n",
    "    binary_output = np.zeros_like(a)\n",
    "    \n",
    "    binary_output[(a >=thresh[0]) & (a <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "def image_processing_pipeline(gray):\n",
    "    sobelx, sobely = take_sobel_op(gray)\n",
    "    r = [\n",
    "        abs_sobel_thresh(orient='x', sobelx=sobelx, sobely=sobely, thresh=(35, 150)),\n",
    "        abs_sobel_thresh(orient='y', sobelx=sobelx, sobely=sobely, thresh=(10, 150)),\n",
    "        mag_thresh(sobelx=sobelx, sobely=sobely, thresh=(50, 200)),\n",
    "        dir_threshold(sobelx=sobelx, sobely=sobely, thresh=(0.7, 1.1))\n",
    "    ]\n",
    "    combined = np.zeros_like(sobelx)\n",
    "    combined[((r[0] == 1) & (r[1] == 1)) | ((r[2] == 1) & (r[3] == 1))] = 1\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def _experiment(gray):\n",
    "    sobelx, sobely = take_sobel_op(gray)\n",
    "    result = []\n",
    "    for i in range(0, 8*10, 10):\n",
    "        result.append(abs_sobel_thresh(orient='y', sobelx=sobelx, sobely=sobely, thresh=(60,60+i)))\n",
    "    print(len(result))\n",
    "    return result\n",
    "        \n",
    "converted = [image_processing_pipeline(gray) for gray in gray_test_images]\n",
    "# converted = _experiment(gray_test_images[0])\n",
    "display_grid(converted, n_col=4, title=\"Images after processing in pipeline\", cmap='gray')\n",
    "# gradient_threshold_images = [gradient_threshold(img, (20, 100)) for img in gray_test_images]\n",
    "# display_grid(gradient_threshold_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we can try to transform colored image from RGB space into HLS, preprocess only Hue and Saturation channels and combine into another binary images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hls_select(img, thresh_s=(0, 255), thresh_h=(0, 255)):\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    \n",
    "    H = hls[:,:,0]\n",
    "    L = hls[:,:,1]\n",
    "    S = hls[:,:,2]\n",
    "\n",
    "    binary_S = np.zeros_like(S)\n",
    "    binary_S[(S > thresh_s[0]) & (S <= thresh_s[1])] = 1\n",
    "\n",
    "    binary_H = np.ones_like(H)\n",
    "    binary_H[(H > thresh_h[0]) & (H <= thresh_h[1])] = 0\n",
    "    \n",
    "    combined = np.zeros_like(S)\n",
    "    combined[(binary_S == 1) & (binary_H == 1)] = 1\n",
    "    \n",
    "    return combined\n",
    "\n",
    "hls_images = [hls_select(img, thresh_s=(150, 255), thresh_h=(150, 255)) for img in test_images]\n",
    "display_grid(hls_images, cmap='gray',n_col=4, title='Images after processing in HLS color space')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these different transformations we can combine them into a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_sobel_and_hls(images):\n",
    "    undistored = undistort_images(images, mtx, dist)\n",
    "    gray_images = [cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) for img in undistored]\n",
    "\n",
    "    converted = [image_processing_pipeline(gray) for gray in gray_images]\n",
    "    hls_images = [hls_select(img, thresh_s=(150, 255), thresh_h=(150, 255)) for img in undistored]\n",
    "\n",
    "    result = []\n",
    "    for sobel_transformed_image, hls_converted_image in zip(converted, hls_images):\n",
    "        combined_binary = np.zeros_like(sobel_transformed_image)\n",
    "        combined_binary[(sobel_transformed_image == 1) | (hls_converted_image == 1)] = 1\n",
    "        result.append(combined_binary)\n",
    "        \n",
    "    return result\n",
    "\n",
    "display_grid(combine_sobel_and_hls(test_images), cmap='gray',n_col=4, title='All transformations combined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "copied_test_images = [img.copy() for img in test_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look what will happen if we do perspective, color and sobel transformations all together. We will produce binary warped images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_warped_images(input_images):\n",
    "    binary_images = combine_sobel_and_hls(input_images)\n",
    "    h, w, *_ = input_images[-1].shape\n",
    "    corners = [(0.45*w, 0.62*h), (0.55*w, 0.62*h), (0.05*w, 0.95*h), (0.95*w, 0.95*h)]\n",
    "    return [transform_perspective(image, corners) \\\n",
    "            for i, image in enumerate(binary_images)]\n",
    "\n",
    "binary_warped = get_binary_warped_images(copied_test_images)\n",
    "display_grid(binary_warped, cmap='gray',n_col=4, title='Binary warped images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task:\n",
    "\n",
    "*Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do lane identification I implement `find_lane` function in `LineDetector` class. The algorithm is simple. At first  we try to identify lanes by using histogram and surfing windows to find out peaks that most probably means to be a line center. That is done only on start to init parameters. At following time frames we will only update positions of lanes pixels filtering them around some area.\n",
    "\n",
    "Given pixels we can try to fit into polynomial curve with help of numpy function `np.polyfit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "*Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've use handy formula to calculate curviture radius when polynomial is known. There are to implementations: `calculate_curviture` that gives curviture in pixels, and `calculate_curviture_in_m` that gives curviture in meters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LineDetector(object):\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    out_img = None\n",
    "    def find_lane(self, binary_warped, draw_frames):\n",
    "        isFirstCall = self.out_img is None\n",
    "        # Create an output image to draw on and  visualize the result\n",
    "        self.out_img = (np.dstack((binary_warped, binary_warped, binary_warped)) * 255).astype(np.uint8)\n",
    "\n",
    "        if not isFirstCall:\n",
    "            self.updated_image(binary_warped)\n",
    "            return\n",
    "        # Assuming you have created a warped binary image called \"binary_warped\"\n",
    "        # Take a histogram of the bottom half of the image\n",
    "        self.input_shape = binary_warped.shape\n",
    "        midpoint = np.int(self.input_shape[0]/2)\n",
    "        histogram = np.sum(binary_warped[midpoint:,:], axis=0)\n",
    "        # Find the peak of the left and right halves of the histogram\n",
    "        # These will be the starting point for the left and right lines\n",
    "        midpoint = np.int(histogram.shape[0]/2)\n",
    "        leftx_base = np.argmax(histogram[:midpoint])\n",
    "        rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "        # Choose the number of sliding windows\n",
    "        nwindows = 9\n",
    "        # Set height of windows\n",
    "        window_height = np.int(self.input_shape[0]/nwindows)\n",
    "        # Identify the x and y positions of all nonzero pixels in the image\n",
    "        nonzero = binary_warped.nonzero()\n",
    "        self.nonzeroy = np.array(nonzero[0])\n",
    "        self.nonzerox = np.array(nonzero[1])\n",
    "        # Current positions to be updated for each window\n",
    "        leftx_current = leftx_base\n",
    "        rightx_current = rightx_base\n",
    "        # Create empty lists to receive left and right lane pixel indices\n",
    "        self.left_lane_inds = []\n",
    "        self.right_lane_inds = []\n",
    "\n",
    "        # Step through the windows one by one\n",
    "        for window in range(nwindows):\n",
    "            # Identify window boundaries in x and y (and right and left)\n",
    "            win_y_low = self.input_shape[0] - (window+1)*window_height\n",
    "            win_y_high = self.input_shape[0] - window*window_height\n",
    "            win_xleft_low = leftx_current - self.margin\n",
    "            win_xleft_high = leftx_current + self.margin\n",
    "            win_xright_low = rightx_current - self.margin\n",
    "            win_xright_high = rightx_current + self.margin\n",
    "            if draw_frames:\n",
    "                # Draw the windows on the visualization image\n",
    "                cv2.rectangle(self.out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),\n",
    "                (0,255,0), 4) \n",
    "                cv2.rectangle(self.out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),\n",
    "                (0,255,0), 4) \n",
    "            # Identify the nonzero pixels in x and y within the window\n",
    "            good_left_inds = ((self.nonzeroy >= win_y_low) & (self.nonzeroy < win_y_high) & \n",
    "            (self.nonzerox >= win_xleft_low) &  (self.nonzerox < win_xleft_high)).nonzero()[0]\n",
    "            good_right_inds = ((self.nonzeroy >= win_y_low) & (self.nonzeroy < win_y_high) & \n",
    "            (self.nonzerox >= win_xright_low) &  (self.nonzerox < win_xright_high)).nonzero()[0]\n",
    "            # Append these indices to the lists\n",
    "            self.left_lane_inds.append(good_left_inds)\n",
    "            self.right_lane_inds.append(good_right_inds)\n",
    "            # If you found > minpix pixels, recenter next window on their mean position\n",
    "            if len(good_left_inds) > self.minpix:\n",
    "                leftx_current = np.int(np.mean(self.nonzerox[good_left_inds]))\n",
    "            if len(good_right_inds) > self.minpix:        \n",
    "                rightx_current = np.int(np.mean(self.nonzerox[good_right_inds]))\n",
    "\n",
    "        # Concatenate the arrays of indices\n",
    "        self.left_lane_inds = np.concatenate(self.left_lane_inds)\n",
    "        self.right_lane_inds = np.concatenate(self.right_lane_inds)\n",
    "\n",
    "        # Extract left and right line pixel positions\n",
    "        leftx = self.nonzerox[self.left_lane_inds]\n",
    "        lefty = self.nonzeroy[self.left_lane_inds] \n",
    "        rightx = self.nonzerox[self.right_lane_inds]\n",
    "        righty = self.nonzeroy[self.right_lane_inds] \n",
    "\n",
    "        # Fit a second order polynomial to each\n",
    "        self.left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        self.right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    def updated_image(self, binary_warped):\n",
    "        nonzero = binary_warped.nonzero()\n",
    "        self.nonzeroy = np.array(nonzero[0])\n",
    "        self.nonzerox = np.array(nonzero[1])\n",
    "        margin = self.margin\n",
    "        self.left_lane_inds = ((self.nonzerox > (self.left_fit[0]*(self.nonzeroy**2) + self.left_fit[1]*self.nonzeroy + \n",
    "        self.left_fit[2] - margin)) & (self.nonzerox < (self.left_fit[0]*(self.nonzeroy**2) + \n",
    "        self.left_fit[1]*self.nonzeroy + self.left_fit[2] + margin))) \n",
    "\n",
    "        self.right_lane_inds = ((self.nonzerox > (self.right_fit[0]*(self.nonzeroy**2) + self.right_fit[1]*self.nonzeroy + \n",
    "        self.right_fit[2] - margin)) & (self.nonzerox < (self.right_fit[0]*(self.nonzeroy**2) + \n",
    "        self.right_fit[1]*self.nonzeroy + self.right_fit[2] + margin)))  \n",
    "\n",
    "        # Again, extract left and right line pixel positions\n",
    "        leftx = self.nonzerox[self.left_lane_inds]\n",
    "        lefty = self.nonzeroy[self.left_lane_inds] \n",
    "        rightx = self.nonzerox[self.right_lane_inds]\n",
    "        righty = self.nonzeroy[self.right_lane_inds]\n",
    "        # Fit a second order polynomial to each\n",
    "        self.left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        self.right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "    def get_out_image(self, **kwargs):\n",
    "        # Generate x and y values for plotting\n",
    "        ploty = np.linspace(0, self.input_shape[0]-1, self.input_shape[0] )\n",
    "        left_fitx = self.left_fit[0]*ploty**2 + self.left_fit[1]*ploty + self.left_fit[2]\n",
    "        right_fitx = self.right_fit[0]*ploty**2 + self.right_fit[1]*ploty + self.right_fit[2]\n",
    "\n",
    "        window_img = np.zeros_like(self.out_img)\n",
    "        self.out_img[self.nonzeroy[self.left_lane_inds], self.nonzerox[self.left_lane_inds]] = (255, 0, 0)\n",
    "        self.out_img[self.nonzeroy[self.right_lane_inds], self.nonzerox[self.right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "        # Generate a polygon to illustrate the search window area\n",
    "        # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "        left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-self.margin, ploty]))])\n",
    "        left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+self.margin, \n",
    "                                      ploty])))])\n",
    "        left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "        right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-self.margin, ploty]))])\n",
    "        right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+self.margin, \n",
    "                                      ploty])))])\n",
    "        right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "        cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "        result = cv2.addWeighted(self.out_img, 1, window_img, 0.3, 0)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def visualize_lines(self):\n",
    "        result = self.get_out_image()\n",
    "        plt.imshow(result)\n",
    "#         plt.plot(left_fitx, ploty, color='yellow')\n",
    "#         plt.plot(right_fitx, ploty, color='yellow')\n",
    "        plt.xlim(0, 1280)\n",
    "        plt.ylim(720, 0)\n",
    "        \n",
    "        \n",
    "\n",
    "    def calculate_curviture_in_m(self):\n",
    "        ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "        xm_per_pix = 3.7/900 # meters per pixel in x dimension\n",
    "\n",
    "        # Fit new polynomials to x,y in world space\n",
    "        ploty = np.linspace(0, 719, num=720)\n",
    "\n",
    "        leftx = self.left_fit[0]*ploty**2 + self.left_fit[1]*ploty + self.left_fit[2]\n",
    "        rightx = self.right_fit[0]*ploty**2 + self.right_fit[1]*ploty + self.right_fit[2]\n",
    "        y_eval = np.max(ploty)\n",
    "\n",
    "        left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "        right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "        # Calculate the new radii of curvature\n",
    "        left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "        right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "        # Now our radius of curvature is in meters\n",
    "\n",
    "        pos = ((rightx[-1] + leftx[-1])/ 2 - 1280 / 2) * xm_per_pix\n",
    "        return left_curverad, right_curverad, pos\n",
    "        \n",
    "    def calculate_curviture(self):\n",
    "        ploty = np.linspace(0, 719, num=720)\n",
    "        y_eval = np.max(ploty)\n",
    "        left_curverad = ((1 + (2*self.left_fit[0]*y_eval + self.left_fit[1])**2)**1.5) / np.absolute(2*self.left_fit[0])\n",
    "        right_curverad = ((1 + (2*self.right_fit[0]*y_eval + self.right_fit[1])**2)**1.5) / np.absolute(2*self.right_fit[0])\n",
    "        return left_curverad, right_curverad\n",
    "    \n",
    "    curviture_stack = []\n",
    "    def print_curviture(self, image):\n",
    "        l, r, p = self.calculate_curviture_in_m()\n",
    "        value = (l+r)/2.\n",
    "        self.curviture_stack.append(value)\n",
    "        if (len(self.curviture_stack) > 20):\n",
    "            self.curviture_stack.pop(0)\n",
    "        mean_value = np.median(self.curviture_stack)\n",
    "        text = \"Curviture: {:.1f} m\".format(mean_value)\n",
    "        cv2.putText(image, text, (10, 100), cv2.FONT_ITALIC, 3.0, (11,71,145), 10, cv2.LINE_AA)\n",
    "        \n",
    "        text = \"Position: {:.2f} m\".format(p)\n",
    "        cv2.putText(image, text, (10, 200), cv2.FONT_ITALIC, 3.0, (11,71,145), 10, cv2.LINE_AA)\n",
    "        \n",
    "    \n",
    "    def restore_lanes_on_image(self, image):\n",
    "        # Create an image to draw the lines on\n",
    "        ploty = np.linspace(0, self.input_shape[0]-1, self.input_shape[0] )\n",
    "        left_fitx = self.left_fit[0]*ploty**2 + self.left_fit[1]*ploty + self.left_fit[2]\n",
    "        right_fitx = self.right_fit[0]*ploty**2 + self.right_fit[1]*ploty + self.right_fit[2]\n",
    "\n",
    "        warp_zero = np.zeros(self.input_shape).astype(np.uint8)\n",
    "        color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "        \n",
    "        # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "        pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "        pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "        pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "        # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "        corners = get_corners(self.input_shape[1], self.input_shape[0])\n",
    "        newwarp = transform_perspective(color_warp, corners, inverted=True)\n",
    "        # Combine the result with the original image\n",
    "        return cv2.addWeighted(image, 1, newwarp, 0.3, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our detector on test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "detected_lines_images = []\n",
    "for image in test_images:\n",
    "    detector = LineDetector()\n",
    "    \n",
    "    input_images = [image]\n",
    "    binary_images = combine_sobel_and_hls(input_images)\n",
    "    h, w, *_ = input_images[-1].shape\n",
    "    corners = get_corners(w, h)\n",
    "    binary_warped = [transform_perspective(image, corners) \\\n",
    "                     for i, image in enumerate(binary_images)]\n",
    "\n",
    "    detector.find_lane(binary_warped[0], draw_frames=False)\n",
    "    image = detector.restore_lanes_on_image(image)\n",
    "    detector.print_curviture(image)\n",
    "    \n",
    "    detected_lines_images.append(image)\n",
    "    \n",
    "display_grid(detected_lines_images, n_col=4, title='Images with identified lanes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Pipeline (video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "*Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tools to make video output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import everything needed to edit/save/watch video clips\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `process_image` method that will accept image, process it and visualize all calculations done - curviture radius, offset from the center of lane and area between lines itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    global detector\n",
    "    input_images = [image]\n",
    "    binary_images = combine_sobel_and_hls(input_images)\n",
    "    h, w, *_ = input_images[-1].shape\n",
    "    corners = get_corners(w, h)\n",
    "    binary_warped = [transform_perspective(image, corners) \\\n",
    "                     for i, image in enumerate(binary_images)]\n",
    "\n",
    "    detector.find_lane(binary_warped[0], draw_frames=False)\n",
    "    image = detector.restore_lanes_on_image(image)\n",
    "    detector.print_curviture(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can export video using our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white_output = 'out/project_video.mp4'\n",
    "del detector\n",
    "detector = LineDetector()\n",
    "\n",
    "## To speed up the testing process you may want to try your pipeline on a shorter subclip of the video\n",
    "## To do so add .subclip(start_second,end_second) to the end of the line below\n",
    "## Where start_second and end_second are integer values representing the start and end of the subclip\n",
    "## You may also uncomment the following line for a subclip of the first 5 seconds\n",
    "##clip1 = VideoFileClip(\"test_videos/solidWhiteRight.mp4\").subclip(0,5)\n",
    "clip1 = VideoFileClip(\"project_video.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "*Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is rather interesting project and helpful in meaning that it describes such new topics for me as color and perspective transformation, Sobel operator and others. It was difficult in process of identifing proper parameters when combining different binary images taken after different transformations. The result is stable without any mistake during the whole video. And I'm very satisfied with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
